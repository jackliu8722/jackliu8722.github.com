<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[博客分类: 区块链 | 精神兵的 Blog]]></title>
  <link href="http://jackliu8722.github.com/blog/categories/blockchain/atom.xml" rel="self"/>
  <link href="http://jackliu8722.github.com/"/>
  <updated>2016-11-29T14:39:37+08:00</updated>
  <id>http://jackliu8722.github.com/</id>
  <author>
    <name><![CDATA[jackliu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[fabric共识机制分析2]]></title>
    <link href="http://jackliu8722.github.com/blog/2016/11/29/fabric-consensus-analysis-2/"/>
    <updated>2016-11-29T11:56:00+08:00</updated>
    <id>http://jackliu8722.github.com/blog/2016/11/29/fabric-consensus-analysis-2</id>
    <content type="html"><![CDATA[<p>在<a href="http://jackliu8722.github.io/blog/2016/10/13/fabric-consensus-analysis-1/">fabric 共识机制分析1</a>中我们介绍了实现共识相关的接口，本文主要介绍fabric中pbft的使用，pbft的上具体算法不是本文的重点，有兴趣可以去拜读pbft的论文。</p>

<h2>1 fabric如何配置使用pbft</h2>

<p>我们知道，fabric目前支持两种共识算法：pbft和noops。当系统启动时怎么知道使用哪种共识算法呢？大家脑子转个0.1s就想到了，肯定是在配置文件中进行配置的呀。恭喜你，猜对了，那配置在哪里呢？首先我们在看看源文件fabric/consensus/conroller/conroller.go中的部分代码。</p>

<p>```go
func NewConsenter(stack consensus.Stack) consensus.Consenter {</p>

<pre><code>plugin := strings.ToLower(viper.GetString("peer.validator.consensus.plugin"))
if plugin == "pbft" {
    logger.Infof("Creating consensus plugin %s", plugin)
    return pbft.GetPlugin(stack)
}
logger.Info("Creating default consensus plugin (noops)")
return noops.GetNoops(stack)
</code></pre>

<p>}
```
从源码中可以看见，具体使用哪种共识算法通过viper获取，viper是fabric依赖第三方go库，这里不做过多的介绍。通过顺藤摸瓜，我们找到peer.validator.consensus.plugin在配置在fabric/peer/core.yaml文件中，大家可以自己去看看，这里就不介绍了。</p>

<h2>2 创建pbft核心引擎</h2>

<p>从上面的代码中我们知道，当plugin为pbft时，调用了方法pbft.GetPlugin，我们看看这个方法做了些什么工作呢？在fabric/consensus/pbft/pbft.go文件找到相应的代码如下：</p>

<p>```go
var pluginInstance consensus.Consenter // 单例
......
func GetPlugin(c consensus.Stack) consensus.Consenter {</p>

<pre><code>if pluginInstance == nil {
    pluginInstance = New(c)
}
return pluginInstance
</code></pre>

<p>}</p>

<p>func New(stack consensus.Stack) consensus.Consenter {</p>

<pre><code>handle, _, _ := stack.GetNetworkHandles()
id, _ := getValidatorID(handle)

switch strings.ToLower(config.GetString("general.mode")) {
case "batch":
    return newObcBatch(id, config, stack)
default:
    panic(fmt.Errorf("Invalid PBFT mode: %s", config.GetString("general.mode")))
}
</code></pre>

<p>}
```
GetPlugin方法创建一个consensus.Consenter的单例，具体的实现调用了New方法，New方法最终委托给newObcBatch方法，newObcBatch方法主要是创建obcBatch对象，并设置相应的属性，我们来看看obcBatch对象的具体定义和newObcBatch的实现:</p>

<p>```go
type obcBatch struct {</p>

<pre><code>obcGeneric
externalEventReceiver //consensus.Consenter接口的实现
pbft        *pbftCore //pbft核心引擎
broadcaster *broadcaster // 广播相关的对象

batchSize        int    //batch的大小
batchStore       []*Request //请求列表
batchTimer       events.Timer
batchTimerActive bool
batchTimeout     time.Duration

manager events.Manager // 事件管理器，后面可能会被移除

incomingChan chan *batchMessage // 正在处理的消息队列
idleChan     chan struct{}     

reqStore *requestStore 

deduplicator *deduplicator

persistForward
</code></pre>

<p>}</p>

<p>func newObcBatch(id uint64, config <em>viper.Viper, stack consensus.Stack) </em>obcBatch {</p>

<pre><code>var err error

op := &amp;obcBatch{
    obcGeneric: obcGeneric{stack: stack},
}

op.persistForward.persistor = stack

logger.Debugf("Replica %d obtaining startup information", id)

op.manager = events.NewManagerImpl() // 创建事件管理器
op.manager.SetReceiver(op)
etf := events.NewTimerFactoryImpl(op.manager)
op.pbft = newPbftCore(id, config, op, etf) // 创建pbft引擎
op.manager.Start()
op.externalEventReceiver.manager = op.manager
op.broadcaster = newBroadcaster(id, op.pbft.N, op.pbft.f, op.pbft.broadcastTimeout, stack)

op.batchSize = config.GetInt("general.batchsize")
op.batchStore = nil
......
op.incomingChan = make(chan *batchMessage)

op.batchTimer = etf.CreateTimer()

op.reqStore = newRequestStore()

op.deduplicator = newDeduplicator()

op.idleChan = make(chan struct{})
close(op.idleChan) // TODO remove eventually

return op
</code></pre>

<p>}
```
从以上代码中我们可以看到，pbft引擎的创建最终调用了newPbftCore，后面我们详细的介绍。</p>

<h2>3 consensus.Consenter接口的实现</h2>

<p>我们知道上节中介绍的New方法要求返回一个consenus.Consenter接口，而newObcBatch返回的是一个obcBatch对象，那说明obcBatch实现了consensus.Consenter接口。obcBatch定义了一个属性externalEventReceiver，该对象实现了consensus.Consenter接口的方法，我们来看看externalEventReceiver的相关源码。</p>

<p>```go
type externalEventReceiver struct {</p>

<pre><code>manager events.Manager
</code></pre>

<p>}</p>

<p>// 当接收到新消息时，用stack调用该方法对消息进行处理
func (eer <em>externalEventReceiver) RecvMsg(ocMsg </em>pb.Message, senderHandle *pb.PeerID) error {</p>

<pre><code>eer.manager.Queue() &lt;- batchMessageEvent{
    msg:    ocMsg,
    sender: senderHandle,
}
return nil
</code></pre>

<p>}
```</p>

<p>externalEventReceiver结构体只包含了一个events.Manager对象，events.Manager是一个事件管理器，后面会详细介绍。RecvMsg方法主要将接收到新消息包装成batchMessageEvent对象通过chan的方法交给了事件管理器处理。externalEventReceiver将新接收到消息交给events.Manager进行处理，包括事务的执行、提交、回滚等事件都交给了events.Manager处理，详细的代码可以在源文件fabric/consensus/pbft/external.go中查看。</p>

<h2>4 事件管理器</h2>

<p>事件管理器相关的实现在源文件fabric/consensus/util/events/events.go中</p>

<p>```go</p>

<p>type Receiver interface {</p>

<pre><code>ProcessEvent(e Event) Event
</code></pre>

<p>}</p>

<p>// 事件管理器接口
type Manager interface {</p>

<pre><code>Inject(Event)         
Queue() chan&lt;- Event 
SetReceiver(Receiver) 
Start()               
Halt()                
</code></pre>

<p>}</p>

<p>// 事件管理器接口的实现
type managerImpl struct {</p>

<pre><code>threaded
receiver Receiver
events   chan Event
</code></pre>

<p>}
```</p>

<p>事件管理器的接口提供了一个Queue方法，该方法返回一个只可写的Event类型的chan，需要处理的事件只需要往这个chan队列上写入事件即可。</p>

<p>事件管理器中有个属性recevier，其类型为Reveiver接口，通过上面第二节的newObcBatch方法我们可以看到，manager的receiver属性被设置成了obcBatch对象，也就是obcBatch实现了Receiver接口的ProcessEvent方法。</p>

<p>我们看再看看事件管理器其它方法的实现：</p>

<p>```go
func (em *managerImpl) Start() {</p>

<pre><code>go em.eventLoop()
</code></pre>

<p>}</p>

<p>func (em *managerImpl) Queue() chan&lt;- Event {</p>

<pre><code>return em.events
</code></pre>

<p>}
func SendEvent(receiver Receiver, event Event) {</p>

<pre><code>next := event
for {
    next = receiver.ProcessEvent(next)
    if next == nil {
        break
    }
}
</code></pre>

<p>}
func (em *managerImpl) Inject(event Event) {</p>

<pre><code>if em.receiver != nil {
    SendEvent(em.receiver, event)
}
</code></pre>

<p>}
func (em *managerImpl) eventLoop() {</p>

<pre><code>for {
    select {
    case next := &lt;-em.events:
        em.Inject(next)
    case &lt;-em.exit:
        logger.Debug("eventLoop told to exit")
        return
    }
}
</code></pre>

<p>}
```
Manager的Start方法主要是启动了一个go的routine去调用eventLoop方法，而eventLoop方法使用了一个无限循环去读取events的事件，当events中有事件需要处理时，便取出一个事件交由Inject方法进行处理，当recevier不为空，Inject方法又调用了SendEvent，而SendEvent方法调用了recevier的ProcessEvent方法对事件进行处理，也就是调用了obcBatch的ProcessEvent方法，这里需要注意了，SendEvent方法里有一个循环，循环结束的条件是ProcessEvent返回的值为空，如果不为空，则继续调用ProcessEvent进行处理，也就是说ProcessEvent处理后可能返回另外另外一个事件。</p>

<h2>5 obcBatch的ProcessEvent方法</h2>

<p>obcBatch的ProcessEvent方法主要根据不同的事件类型，调用相应的处理方法，具体代码如下：</p>

<p>```go
func (op *obcBatch) ProcessEvent(event events.Event) events.Event {</p>

<pre><code>logger.Debugf("Replica %d batch main thread looping", op.pbft.id)
switch et := event.(type) {
case batchMessageEvent:
    ocMsg := et
    return op.processMessage(ocMsg.msg, ocMsg.sender)
case executedEvent:
    op.stack.Commit(nil, et.tag.([]byte))
case committedEvent:
    logger.Debugf("Replica %d received committedEvent", op.pbft.id)
    return execDoneEvent{}
case execDoneEvent:
    if res := op.pbft.ProcessEvent(event); res != nil {
        return res
    }
    return op.resubmitOutstandingReqs()
case batchTimerEvent:
    logger.Infof("Replica %d batch timer expired", op.pbft.id)
    if op.pbft.activeView &amp;&amp; (len(op.batchStore) &gt; 0) {
        return op.sendBatch()
    }
case *Commit:
    res := op.pbft.ProcessEvent(event)
    op.startTimerIfOutstandingRequests()
    return res
case viewChangedEvent:
    op.batchStore = nil
    op.pbft.outstandingReqBatches = make(map[string]*RequestBatch)

    logger.Debugf("Replica %d batch thread recognizing new view", op.pbft.id)
    if op.batchTimerActive {
        op.stopBatchTimer()
    }

    if op.pbft.skipInProgress {
        op.reqStore.outstandingRequests.empty()
    }

    op.reqStore.pendingRequests.empty()
    for i := op.pbft.h + 1; i &lt;= op.pbft.h+op.pbft.L; i++ {
        if i &lt;= op.pbft.lastExec {
            continue
        }

        cert, ok := op.pbft.certStore[msgID{v: op.pbft.view, n: i}]
        if !ok || cert.prePrepare == nil {
            continue
        }

        if cert.prePrepare.BatchDigest == "" {
            continue
        }

        if cert.prePrepare.RequestBatch == nil {
            logger.Warningf("Replica %d found a non-null prePrepare with no request batch, ignoring")
            continue
        }

        op.reqStore.storePendings(cert.prePrepare.RequestBatch.GetBatch())
    }

    return op.resubmitOutstandingReqs()
case stateUpdatedEvent:
    op.reqStore = newRequestStore()
    return op.pbft.ProcessEvent(event)
default:
    return op.pbft.ProcessEvent(event)
}

return nil
</code></pre>

<p>}
```</p>

<p>我们先看看事件类型为batchMessageEvent的情况，当事件类型为batchMessageEvent时，ProcessEvent方法最终调用了obcBatch的processMessage方法，而processMessage方法主要处理两类消息：pb.Message_CHAIN_TRANSACTION和pb.Message_CONSENSUS，代码如下：</p>

<p>```go
func (op <em>obcBatch) processMessage(ocMsg </em>pb.Message, senderHandle *pb.PeerID) events.Event {</p>

<pre><code>// 处理事务事件
if ocMsg.Type == pb.Message_CHAIN_TRANSACTION {
    req := op.txToReq(ocMsg.Payload)
    return op.submitToLeader(req)
}
// 处理共识消息事件
if ocMsg.Type != pb.Message_CONSENSUS {
    logger.Errorf("Unexpected message type: %s", ocMsg.Type)
    return nil
}

batchMsg := &amp;BatchMessage{}
err := proto.Unmarshal(ocMsg.Payload, batchMsg)
if err != nil {
    logger.Errorf("Error unmarshaling message: %s", err)
    return nil
}

if req := batchMsg.GetRequest(); req != nil {
    if !op.deduplicator.IsNew(req) {
        logger.Warningf("Replica %d ignoring request as it is too old", op.pbft.id)
        return nil
    }

    op.logAddTxFromRequest(req)
    op.reqStore.storeOutstanding(req)
    if (op.pbft.primary(op.pbft.view) == op.pbft.id) &amp;&amp; op.pbft.activeView {
        return op.leaderProcReq(req)
    }
    op.startTimerIfOutstandingRequests()
    return nil
} else if pbftMsg := batchMsg.GetPbftMessage(); pbftMsg != nil {
    senderID, err := getValidatorID(senderHandle) // who sent this?
    if err != nil {
        panic("Cannot map sender's PeerID to a valid replica ID")
    }
    msg := &amp;Message{}
    err = proto.Unmarshal(pbftMsg, msg)
    if err != nil {
        logger.Errorf("Error unpacking payload from message: %s", err)
        return nil
    }
    return pbftMessageEvent{
        msg:    msg,
        sender: senderID,
    }
}

logger.Errorf("Unknown request: %+v", batchMsg)

return nil
</code></pre>

<p>}</p>

<p>// 将事务消息转换为Request对象
func (op <em>obcBatch) txToReq(tx []byte) </em>Request {</p>

<pre><code>now := time.Now()
req := &amp;Request{
    Timestamp: &amp;google_protobuf.Timestamp{
        Seconds: now.Unix(),
        Nanos:   int32(now.UnixNano() % 1000000000),
    },
    Payload:   tx,
    ReplicaId: op.pbft.id,
}
// XXX sign req
return req
</code></pre>

<p>}
```</p>

<p>当消息类型为pb.Message_CHAIN_TRANSACTION时，通过调用obcBatch的txToReq方法将消息转换为Request对象，然后调用submitToLeader方法将消息进行广播，通过跟踪源码发现，如果当前节点是leader，则将消息封装成RequestBatch对象并返回；当消息类型为pb.Message_CONSENSUS时，通过分析源码，最终processEvent方法可能会返回RequestBatch对象、pbftMessageEvent对象或者nil。因此，在ProcessEvent方法中，当事件类型为batchMessageEvent时，ProcessEvent可能返回RequestBatch、pbftMessageEvent事件或者nil，通过上节的分析我们知道，当ProcessEvent返回不为空时，会将返回值作为参数继续调用ProcessEvent进行处理。通过分析ProcessEvent方法我们知道，当事件类型为RequestBatch和pbftMessageEvent时，调用了obcBatch.pbft.ProcessEvent方法进行处理，pbft的ProcessEvent方法是pbft共识机制的入口，后面会进行详细的分析。</p>

<h2>6 pbft核心引擎</h2>

<p>pbft引擎的核心对象是pbftCore，存储了pbft算法相关的参数和状态，具体的代码如下：</p>

<p>```go
type pbftCore struct {</p>

<pre><code>...
// PBFT data
activeView    bool              // 当前view是否处于活动状态
byzantine     bool              // 当前节点是否为拜占庭节点
f             int               // 最大能容忍失败的节点
N             int               // 网络中最大的合法节点数
h             uint64            
id            uint64            // 节点id
K             uint64            // 检查点周期
logMultiplier uint64            
L             uint64           
lastExec      uint64            // 最后一次执行的命令id
replicaCount  int               // 副本数
seqNo         uint64            // 单调递增的序列号
view          uint64            // 当前的view
chkpts        map[uint64]string 
pset          map[uint64]*ViewChange_PQ // 当前节点上一个view里到达pre-prepared状态的请求的一些信息
qset          map[qidx]*ViewChange_PQ // 当前节点在上一个view中达到prepared状态的请求的一些信息
...

currentExec           *uint64                  // 正在执行的命令
...

certStore       map[msgID]*msgCert       // 存储每个请求每个阶段的投票情况 
...
</code></pre>

<p>}</p>

<p>type msgCert struct {</p>

<pre><code>digest      string
prePrepare  *PrePrepare
sentPrepare bool
prepare     []*Prepare
sentCommit  bool
commit      []*Commit
</code></pre>

<p>}
```</p>

<p>pbftCore对象的结构就按照pbft算法要求设计的，具体的算法不是本文的重点，需要研究pbft算法的，请看pbft的原始论文。</p>

<p>上节中我们分析到，pbft算法的入口pbftCore的ProcessEvent方法，我们来看看该方法具体做了些什么。</p>

<p>```go
func (instance *pbftCore) ProcessEvent(e events.Event) events.Event {</p>

<pre><code>var err error
logger.Debugf("Replica %d processing event", instance.id)
switch et := e.(type) {
case viewChangeTimerEvent:
    logger.Infof("Replica %d view change timer expired, sending view change: %s", instance.id, instance.newViewTimerReason)
    instance.timerActive = false
    instance.sendViewChange()
case *pbftMessage:
    return pbftMessageEvent(*et)
case pbftMessageEvent:
    msg := et
    logger.Debugf("Replica %d received incoming message from %v", instance.id, msg.sender)
    next, err := instance.recvMsg(msg.msg, msg.sender)
    if err != nil {
        break
    }
    return next
case *RequestBatch:
    err = instance.recvRequestBatch(et)
case *PrePrepare:
    err = instance.recvPrePrepare(et)
case *Prepare:
    err = instance.recvPrepare(et)
case *Commit:
    err = instance.recvCommit(et)
case *Checkpoint:
    return instance.recvCheckpoint(et)
case *ViewChange:
    return instance.recvViewChange(et)
case *NewView:
    return instance.recvNewView(et)
case *FetchRequestBatch:
    err = instance.recvFetchRequestBatch(et)
case returnRequestBatchEvent:
    return instance.recvReturnRequestBatch(et)
case stateUpdatedEvent:
    update := et.chkpt
    instance.stateTransferring = false
    // If state transfer did not complete successfully, or if it did not reach our low watermark, do it again
    if et.target == nil || update.seqNo &lt; instance.h {
        if et.target == nil {
            logger.Warningf("Replica %d attempted state transfer target was not reachable (%v)", instance.id, et.chkpt)
        } else {
            logger.Warningf("Replica %d recovered to seqNo %d but our low watermark has moved to %d", instance.id, update.seqNo, instance.h)
        }
        if instance.highStateTarget == nil {
            logger.Debugf("Replica %d has no state targets, cannot resume state transfer yet", instance.id)
        } else if update.seqNo &lt; instance.highStateTarget.seqNo {
            logger.Debugf("Replica %d has state target for %d, transferring", instance.id, instance.highStateTarget.seqNo)
            instance.retryStateTransfer(nil)
        } else {
            logger.Debugf("Replica %d has no state target above %d, highest is %d", instance.id, update.seqNo, instance.highStateTarget.seqNo)
        }
        return nil
    }
    logger.Infof("Replica %d application caught up via state transfer, lastExec now %d", instance.id, update.seqNo)
    // XXX create checkpoint
    instance.lastExec = update.seqNo
    instance.moveWatermarks(instance.lastExec) // The watermark movement handles moving this to a checkpoint boundary
    instance.skipInProgress = false
    instance.consumer.validateState()
    instance.executeOutstanding()
case execDoneEvent:
    instance.execDoneSync()
    if instance.skipInProgress {
        instance.retryStateTransfer(nil)
    }
    // We will delay new view processing sometimes
    return instance.processNewView()
case nullRequestEvent:
    instance.nullRequestHandler()
case workEvent:
    et() // Used to allow the caller to steal use of the main thread, to be removed
case viewChangeQuorumEvent:
    logger.Debugf("Replica %d received view change quorum, processing new view", instance.id)
    if instance.primary(instance.view) == instance.id {
        return instance.sendNewView()
    }
    return instance.processNewView()
case viewChangedEvent:
    // No-op, processed by plugins if needed
case viewChangeResendTimerEvent:
    if instance.activeView {
        logger.Warningf("Replica %d had its view change resend timer expire but it's in an active view, this is benign but may indicate a bug", instance.id)
        return nil
    }
    logger.Debugf("Replica %d view change resend timer expired before view change quorum was reached, resending", instance.id)
    instance.view-- // sending the view change increments this
    return instance.sendViewChange()
default:
    logger.Warningf("Replica %d received an unknown message type %T", instance.id, et)
}

if err != nil {
    logger.Warning(err.Error())
}

return nil
</code></pre>

<p>}
```</p>

<p>这么多代码，是不是被吓着了，偷笑中...。其实ProcessEvent没有做太复杂的工作，仅仅是根据不同的事件类型调用不同的方法，包括处理pre-prepare、prepare、commit、viewChange等事件，熟悉pbft算法的同学直接看源码就行，源码比较清晰，这里就不多介绍了。</p>

<h2>7 总结</h2>

<p>本文主要带大家粗略地过了一下fabric中使用pbft实现共识机制的过程，为大家提供一个分析共识机制源码的架子。当然还有未涉及的地方，如消息广播的实现等。有了这个架子大家便可以快速的对源码进行分析或者修改，希望对大家有帮助。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fabric共识机制分析1]]></title>
    <link href="http://jackliu8722.github.com/blog/2016/10/13/fabric-consensus-analysis-1/"/>
    <updated>2016-10-13T18:25:00+08:00</updated>
    <id>http://jackliu8722.github.com/blog/2016/10/13/fabric-consensus-analysis-1</id>
    <content type="html"><![CDATA[<p>共识机制的实现提供了一系列的接口，下列首先对接口进行一下简单的介绍，详见注释。</p>

<h2>1 ExecutionConsumer接口</h2>

<p>```go
// ExecutionConsumer接口实现异步回调的功能
type ExecutionConsumer interface {</p>

<pre><code>Executed(tag interface{})                                // 事务成功执行完成后调用
Committed(tag interface{}, target *pb.BlockchainInfo)    // 事务成功commit之后调用
RolledBack(tag interface{})                              // 事务成功回滚之后调用
StateUpdated(tag interface{}, target *pb.BlockchainInfo) // 状态转换完成之后调用,如果参数为nil，说明转换失败，并要提供新的target
</code></pre>

<p>}
```</p>

<p>该接口中用到了pb.BlockchainInfo，pb.BlockchainInfo是通过protobuf定义生成，表示区块的信息，其详细定义如下：</p>

<p>```go
// 区块链账本的信息：高度、当前区块的hash、前一个区块的hash
type BlockchainInfo struct {</p>

<pre><code>Height            uint64 `protobuf:"varint,1,opt,name=height" json:"height,omitempty"`
CurrentBlockHash  []byte `protobuf:"bytes,2,opt,name=currentBlockHash,proto3" json:"currentBlockHash,omitempty"`
PreviousBlockHash []byte `protobuf:"bytes,3,opt,name=previousBlockHash,proto3" json:"previousBlockHash,omitempty"`
</code></pre>

<p>}
```</p>

<h2>2 Consenter接口</h2>

<p>```go
type Consenter interface {</p>

<pre><code>RecvMsg(msg *pb.Message, senderHandle *pb.PeerID) error
ExecutionConsumer
</code></pre>

<p>}
```
Consenter接口用于从网络上接上消息，每个共识plugin必须实现该接口。当其它节点通过gRPC发送消息时，当前节点对接受到的消息依次调用RecvMsg方法进行处理，该方法需要提供两个参数：消息内容和发送节点的信息，对应的结构类分别为pb.Message和pb.PeerID，其详细定义见如下代码和注释：</p>

<p>```go
// 消息结构类
type Message struct {</p>

<pre><code>// 消息类型
Type      Message_Type               `protobuf:"varint,1,opt,name=type,enum=protos.Message_Type" json:"type,omitempty"`
// 时间
Timestamp *google_protobuf.Timestamp `protobuf:"bytes,2,opt,name=timestamp" json:"timestamp,omitempty"`
// 消息内容
Payload   []byte                     `protobuf:"bytes,3,opt,name=payload,proto3" json:"payload,omitempty"`
// 消息签名
Signature []byte                     `protobuf:"bytes,4,opt,name=signature,proto3" json:"signature,omitempty"`
</code></pre>

<p>}</p>

<p>// 发送者节点信息
type PeerID struct {</p>

<pre><code>// 发送者名称
Name string `protobuf:"bytes,1,opt,name=name" json:"name,omitempty"`
</code></pre>

<p>}
```
Consenter接口还包含了ExecutionConsumer接口的方法，这里就不多介绍了。</p>

<h2>3 Inquirer接口</h2>

<p>Inquirer接口主要是获取网络节点信息，详细定义如下：</p>

<p>```go
type Inquirer interface {</p>

<pre><code>// 获取节点信息
GetNetworkInfo() (self *pb.PeerEndpoint, network []*pb.PeerEndpoint, err error)
// 获取节点ID
GetNetworkHandles() (self *pb.PeerID, network []*pb.PeerID, err error)
</code></pre>

<p>}
```
pg.PeerEndpoint存储了节点的信息，其定义如下：</p>

<p>```go
type PeerEndpoint struct {</p>

<pre><code>// 节点ID
ID      *PeerID           `protobuf:"bytes,1,opt,name=ID" json:"ID,omitempty"`
// 节点地址 ip:port
Address string            `protobuf:"bytes,2,opt,name=address" json:"address,omitempty"`
// 节点类型
Type    PeerEndpoint_Type `protobuf:"varint,3,opt,name=type,enum=protos.PeerEndpoint_Type" json:"type,omitempty"`
// 公钥ID
PkiID   []byte            `protobuf:"bytes,4,opt,name=pkiID,proto3" json:"pkiID,omitempty"`
</code></pre>

<p>}
```</p>

<h2>4 Communicator接口</h2>

<p>Communicator接口提供了向节点广播和单播消息的方法，具体定义如下：</p>

<p>```go
type Communicator interface {</p>

<pre><code>// 向网络中的其它节点广播消息
Broadcast(msg *pb.Message, peerType pb.PeerEndpoint_Type) error
// 向指定的节点发送消息
Unicast(msg *pb.Message, receiverHandle *pb.PeerID) error
</code></pre>

<p>}
```</p>

<h2>5 NetworkStack接口</h2>

<p>NetworkStack接口的方法来自于Communicator接口和Inquirer接口，主要是发送消息和获取消息的一个封装。</p>

<p>```go
type NetworkStack interface {</p>

<pre><code>Communicator
Inquirer
</code></pre>

<p>}
```</p>

<h2>6 SecurityUtils接口</h2>

<p>SecurityUtils接口用于对消息进行签名和验证。</p>

<p>```go
type SecurityUtils interface {</p>

<pre><code>Sign(msg []byte) ([]byte, error)
Verify(peerID *pb.PeerID, signature []byte, message []byte) error
</code></pre>

<p>}
```</p>

<h2>7 ReadOnlyLedger接口</h2>

<p>ReadOnlyLedger接口用于访问区块链的信息，只能进行读取不能进行写操作，详细定义如下：</p>

<p>```go
type ReadOnlyLedger interface {</p>

<pre><code>// 根据ID获取一个区块
GetBlock(id uint64) (block *pb.Block, err error)
// 获取区块的大小
GetBlockchainSize() uint64
// 获取区块的高度和hash等信息
GetBlockchainInfo() *pb.BlockchainInfo
// 获取区块的数据
GetBlockchainInfoBlob() []byte
// 获取区块的haader的meta信息
GetBlockHeadMetadata() ([]byte, error)
</code></pre>

<p>}
```
pb.Block表示一个区块，包括版本、hash值、transaction等，具体这里就不多介绍了。</p>

<h2>8 LegacyExecutor接口</h2>

<p>LegacyExecutor接口用于执行事务相关的保重，会修改账本的内容，提供的方法如下：</p>

<p>```go
type LegacyExecutor interface {</p>

<pre><code>// 开启一个事务，并提供事务id
BeginTxBatch(id interface{}) error
// 执行事务id
ExecTxs(id interface{}, txs []*pb.Transaction) ([]byte, error)
// 提交事务id
CommitTxBatch(id interface{}, metadata []byte) (*pb.Block, error)
// 回滚事务id
RollbackTxBatch(id interface{}) error
// 事务提交前的预览
PreviewCommitTxBatch(id interface{}, metadata []byte) ([]byte, error)
</code></pre>

<p>}
```</p>

<h2>9 Executor接口</h2>

<p>Executor接口最终会替换掉老的事务执行接口，也就是LegacyExecutor接口。调用LegacyExecutor接口进行事务操作时，为了避免资源竞争和损坏账本，必须与状态转移进行协调。（个人理解就是需要锁来进行协调）</p>

<p>```go
type Executor interface {</p>

<pre><code>// 获取所需要的资源
Start()         
// 释放所占有的资源                            
Halt()        
// 执行一系列的事务                           
Execute(tag interface{}, txs []*pb.Transaction)
// 事务执行完成之后进行提交
Commit(tag interface{}, metadata []byte)  
// 事务的回滚操作
Rollback(tag interface{})  
// 更新状态
UpdateState(tag interface{}, target *pb.BlockchainInfo, peers []*pb.PeerID)
</code></pre>

<p>}
```</p>

<h2>10 LedgerManager接口</h2>

<p>LedgerManager接口用于管理账本的状态。</p>

<p>```go
type LedgerManager interface {</p>

<pre><code>// 使账本为无效状态，在该状态应该拒绝查询
InvalidateState()
// 使用账本为有效状态，当前状态下应该恢复为可提供查询操作
ValidateState()
</code></pre>

<p>}
```</p>

<h2>11 StatePersistor接口</h2>

<p>StatePersistor接口用于保存共识状态，防止进程崩溃丢失状态。</p>

<p>```go
type StatePersistor interface {</p>

<pre><code>// 存储状态
StoreState(key string, value []byte) error
// 读取状态
ReadState(key string) ([]byte, error)
// 按前缀读取状态信息
ReadStateSet(prefix string) (map[string][]byte, error)
// 删除状态
DelState(key string)
</code></pre>

<p>}
```</p>

<h2>12 Stack接口</h2>

<p>Stack接口仅仅是对共识plugin需要实现的接口进行了一层封装，具体如下：</p>

<p>```go
type Stack interface {</p>

<pre><code>NetworkStack
SecurityUtils
Executor
LegacyExecutor
LedgerManager
ReadOnlyLedger
StatePersistor
</code></pre>

<p>}
```</p>

<p>以上是对consensus的接口的介绍，目前fabric提供了两种共识算法的实现，分别是noops和pbft，noops主要是针对单节点的实现，主要用于测试；而pbft是一种拜占庭容错算法，用于多节点的共识机制。</p>

<p>fabric多节点的共识机制使用的是pbft。</p>
]]></content>
  </entry>
  
</feed>
